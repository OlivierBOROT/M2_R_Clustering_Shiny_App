---
title: "Complete Guide to DivisiveClusterer"
subtitle: "Divisive Variable Clustering (VARCLUS-style)"
author: "M2RClust Team"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Complete Guide to DivisiveClusterer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)
```

## Introduction

The `DivisiveClusterer` implements a **divisive (top-down) hierarchical clustering** algorithm for variables, inspired by VARCLUS (SAS). Unlike the `KMeansClusterer` which starts with K clusters and optimizes them, this algorithm:

1. Starts with all variables in a single cluster
2. Iteratively splits the most heterogeneous cluster
3. Stops when the desired number of clusters is reached (or based on a stopping criterion)

### Advantages of Divisive Clustering

- **Deterministic**: No random initialization, 100% reproducible results
- **Interpretable hierarchy**: Complete history of splits
- **Automatic stopping criteria**: Kaiser rule, eigenvalue ratio
- **Hybrid**: Fast path for numeric data, PCAmix for mixed data

### Algorithm Principle

At each iteration:

1. **Selection**: Identify the cluster with the largest 2nd eigenvalue (λ₂)
2. **Verification**: Check stopping criteria (Kaiser, ratio)
3. **Division**: PCA on the cluster, Varimax rotation, assignment based on squared loadings
4. **Update**: Record the split history

## Installation and Loading

```{r load-package}
library(M2RClust)
```

## Example Data

```{r data}
data(mtcars)

# Larger data to better illustrate the hierarchy
set.seed(123)
n <- 100
p <- 12

# Create data with known cluster structure
data_struct <- data.frame(
  # Group 1: Correlated variables (performance)
  V1 = rnorm(n),
  V2 = NA, V3 = NA,
  # Group 2: Correlated variables (dimensions)
  V4 = rnorm(n),
  V5 = NA, V6 = NA,
  # Group 3: Correlated variables (economy)
  V7 = rnorm(n),
  V8 = NA, V9 = NA,
  # Group 4: Independent variables
  V10 = rnorm(n),
  V11 = rnorm(n),
  V12 = rnorm(n)
)

# Add within-group correlations
data_struct$V2 <- data_struct$V1 * 0.9 + rnorm(n, sd = 0.3)
data_struct$V3 <- data_struct$V1 * 0.85 + rnorm(n, sd = 0.4)
data_struct$V5 <- data_struct$V4 * 0.88 + rnorm(n, sd = 0.35)
data_struct$V6 <- data_struct$V4 * 0.82 + rnorm(n, sd = 0.45)
data_struct$V8 <- data_struct$V7 * 0.87 + rnorm(n, sd = 0.38)
data_struct$V9 <- data_struct$V7 * 0.80 + rnorm(n, sd = 0.5)

cat("Data dimensions:", dim(data_struct), "\n")
```

## Basic Usage

### Creating and Fitting

```{r basic-usage}
# Create the divisive clusterer
clusterer <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 4,
  standardize = TRUE
)

# Fit
clusterer$fit()

# Display
clusterer$print()
```

### Detailed Summary

```{r summary}
clusterer$summary()
```

## Split History

One of the major advantages of `DivisiveClusterer` is access to the complete split history.

### Consulting the History

```{r split-history}
history <- clusterer$get_split_history()

cat("=== Split History ===\n\n")
for (h in history) {
  cat(sprintf("Iteration %d:\n", h$iteration))
  cat(sprintf("  Split cluster: %d\n", h$split_cluster))
  cat(sprintf("  λ₁ = %.3f, λ₂ = %.3f\n", h$eigenvalue_1, h$eigenvalue_2))
  cat(sprintf("  Ratio λ₂/λ₁ = %.3f\n", h$eigenvalue_ratio))
  cat(sprintf("  Split quality: %.3f\n", h$split_quality))
  cat(sprintf("  Group 1: %s\n", paste(h$variables_group1, collapse = ", ")))
  cat(sprintf("  Group 2: %s\n", paste(h$variables_group2, collapse = ", ")))
  cat("\n")
}
```

### Summary Table

```{r split-details}
details <- clusterer$get_split_details()
print(details)
```

## Dendrogram Visualization

```{r dendrogram, fig.cap="Split tree (top-down dendrogram)", fig.height=6}
clusterer$plot_split_dendrogram(
  main = "Divisive Clustering Tree",
  show_eigenvalues = TRUE,
  show_vars = TRUE
)
```

## Stopping Criteria

### Kaiser Criterion

The Kaiser criterion stops splitting when λ₂ < 1 (no significant secondary component).

```{r kaiser}
clusterer_kaiser <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 10,  # Theoretical maximum
  stop_at_kaiser = TRUE
)
clusterer_kaiser$fit()

cat("Clusters formed with Kaiser:", max(clusterer_kaiser$clusters), "\n")
clusterer_kaiser$print()
```

### Eigenvalue Ratio

Stops when λ₂/λ₁ falls below a threshold (cluster already homogeneous).

```{r eigenvalue-ratio}
clusterer_ratio <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 10,
  min_eigenvalue_ratio = 0.2  # Stop if λ₂/λ₁ < 0.2
)
clusterer_ratio$fit()

cat("Clusters formed with ratio 0.2:", max(clusterer_ratio$clusters), "\n")
```

### Minimum Cluster Size

Prevents splitting clusters that are too small.

```{r min-size}
clusterer_minsize <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 6,
  min_cluster_size = 3  # At least 3 variables to split
)
clusterer_minsize$fit()
clusterer_minsize$print()
```

## Rotation Methods

### Varimax (default)

Orthogonal rotation that maximizes the variance of squared loadings.

```{r rotation-varimax}
clust_varimax <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 4,
  rotation_method = "varimax"
)
clust_varimax$fit()
cat("Homogeneity (Varimax):", round(clust_varimax$get_homogeneity(), 4), "\n")
```

### Promax

Oblique rotation, allows correlated factors.

```{r rotation-promax}
clust_promax <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 4,
  rotation_method = "promax",
  promax_m = 4  # Power parameter
)
clust_promax$fit()
cat("Homogeneity (Promax):", round(clust_promax$get_homogeneity(), 4), "\n")
```

### No Rotation

```{r rotation-none}
clust_none <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 4,
  rotation_method = "none"
)
clust_none$fit()
cat("Homogeneity (No rotation):", round(clust_none$get_homogeneity(), 4), "\n")
```

## Cluster Selection Criterion

### By λ₂ (default)

Splits the cluster with the largest 2nd eigenvalue.

```{r split-eigenvalue2}
clust_ev2 <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 4,
  split_criterion = "eigenvalue2"
)
clust_ev2$fit()
```

### By Homogeneity

Splits the least homogeneous cluster (1 - λ₁/(λ₁+λ₂)).

```{r split-homogeneity}
clust_homog <- DivisiveClusterer$new(
  data = data_struct,
  n_clusters = 4,
  split_criterion = "homogeneity"
)
clust_homog$fit()
```

## Visualizations

The `DivisiveClusterer` has specific visualizations that are well-suited to its hierarchical nature:

### Best Visualizations for Divisive Clustering

| Visualization | Relevance | Why |
|--------------|-----------|-----|
| **Dendrogram** | ⭐⭐⭐ | Shows the split history - signature plot |
| **Correlation Heatmap** | ⭐⭐⭐ | Reveals block structure by cluster |
| **Cluster Quality** | ⭐⭐ | Homogeneity is the optimization criterion |
| **Network Graph** | ⭐⭐ | Visualizes variable relationships |
| Correlation Circle | ⭐ | Global PCA doesn't reflect split logic |

### Correlation Heatmap (Recommended)

The heatmap reordered by cluster clearly shows the block-diagonal structure that the algorithm discovers:

```{r plot-heatmap, fig.cap="Correlation heatmap showing cluster structure", fig.height=6}
plot_correlation_heatmap(clusterer, reorder = TRUE)
```

### Cluster Quality Metrics

Shows cluster sizes and homogeneity (λ₁ / Σλ) - the criterion optimized at each split:

```{r plot-quality, fig.cap="Cluster sizes and homogeneity"}
plot_cluster_quality(clusterer)
```

### Network Graph (if igraph available)

```{r plot-network, fig.cap="Variable correlation network", eval=requireNamespace("igraph", quietly=TRUE)}
plot_network_graph(clusterer, threshold = 0.5)
```

### Note on Correlation Circle

The correlation circle (`plot_clustering_2d()`) uses a global PCA which doesn't naturally reflect the successive splits of the divisive algorithm. It can still be used for comparison with KMeansClusterer, but the dendrogram is more informative for understanding the hierarchical structure.

## Prediction on New Variables

```{r predict}
# New variables
new_vars <- data.frame(
  NewV1 = data_struct$V1 * 0.7 + rnorm(n, sd = 0.5),  # Close to group 1
  NewV2 = data_struct$V4 * 0.6 + rnorm(n, sd = 0.6),  # Close to group 2
  NewV3 = rnorm(n)  # Independent
)

predictions <- clusterer$predict(new_vars)
print(predictions)
```

## Selecting the Number of Clusters

The `DivisiveClusterer` is compatible with K selection methods.

### Elbow Method

```{r elbow-divisive, fig.cap="Elbow method - DivisiveClusterer"}
elbow_res <- elbow_method(
  DivisiveClusterer,
  data_struct,
  k_range = 2:8,
  plot = TRUE
)
cat("Suggested K:", elbow_res$suggested_k, "\n")
```

### Complete Comparison

```{r compare-k-divisive, fig.width=12, fig.height=4}
comparison <- compare_k_selection_methods(
  DivisiveClusterer,
  data_struct,
  k_range = 2:7,
  plot = TRUE
)
cat("Consensus K:", comparison$consensus_k, "\n")
```

## Mixed Data

The `DivisiveClusterer` automatically detects the data type and uses:
- **Fast path**: `eigen(cor(X))` for purely numeric data
- **PCAmix**: For mixed data (numeric + categorical)

```{r mixed-data}
# Mixed data
data_mixed <- data_struct[, 1:6]
data_mixed$Cat1 <- factor(sample(c("A", "B", "C"), n, replace = TRUE))
data_mixed$Cat2 <- factor(sample(c("X", "Y"), n, replace = TRUE))

str(data_mixed)

if (requireNamespace("PCAmixdata", quietly = TRUE)) {
  clusterer_mixed <- DivisiveClusterer$new(
    data = data_mixed,
    n_clusters = 3
  )
  clusterer_mixed$fit()
  clusterer_mixed$print()
}
```

## Comparison with KMeansClusterer

```{r compare-methods}
# Same data, same parameters (use numeric columns only)
data(mtcars)
data_test <- mtcars[, c("mpg", "cyl", "disp", "hp", "wt", "qsec")]

# DivisiveClusterer
div_clust <- DivisiveClusterer$new(data = data_test, n_clusters = 3)
div_clust$fit()

# KMeansClusterer
km_clust <- KMeansClusterer$new(data = data_test, n_clusters = 3, seed = 42)
km_clust$fit()

cat("=== Comparison ===\n")
cat(sprintf("DivisiveClusterer - Homogeneity: %.4f\n", div_clust$get_homogeneity()))
cat(sprintf("KMeansClusterer   - Homogeneity: %.4f\n", km_clust$get_homogeneity()))

# Compare assignments
cat("\nDivisiveClusterer assignments:\n")
print(table(div_clust$clusters))

cat("\nKMeansClusterer assignments:\n")
print(table(km_clust$clusters))
```

### Side-by-Side Visualization

```{r compare-viz, fig.width=12, fig.height=5}
par(mfrow = c(1, 2))
plot_clustering_2d(div_clust, main = "DivisiveClusterer")
plot_clustering_2d(km_clust, main = "KMeansClusterer")
par(mfrow = c(1, 1))
```

## Recommended Use Cases

### When to Use DivisiveClusterer?

1. **Absolute reproducibility**: 100% deterministic results
2. **Hierarchical interpretation**: Understanding how groups form
3. **Automatic stopping**: Let the algorithm determine optimal K
4. **Large datasets**: The fast path is very efficient

### When to Prefer KMeansClusterer?

1. **Fixed and known K**: Direct optimization for K clusters
2. **Initialization flexibility**: Testing multiple starts
3. **Highly heterogeneous data**: Better convergence to global optimum

## Export and Integration

### Retrieving Assignments

```{r export-assignments}
assignments <- clusterer$get_cluster_assignments()
head(assignments)
```

### Retrieving the Hierarchical Structure

```{r export-tree}
tree <- clusterer$get_hierarchy_tree()
str(tree, max.level = 2)
```

### Cluster Centers

```{r export-centers}
centers <- clusterer$get_centers()
cat("Number of centers:", length(centers), "\n")
cat("Length of first center:", length(centers[[1]]), "observations\n")
```

## Advanced Parameters - Summary

| Parameter | Default | Description |
|-----------|---------|-------------|
| `n_clusters` | - | Maximum number of clusters |
| `standardize` | `TRUE` | Standardize the data |
| `min_cluster_size` | `3` | Minimum size to split |
| `rotation_method` | `"varimax"` | `"varimax"`, `"promax"`, `"none"` |
| `split_criterion` | `"eigenvalue2"` | `"eigenvalue2"` or `"homogeneity"` |
| `stop_at_kaiser` | `FALSE` | Stop if λ₂ < 1 |
| `min_eigenvalue_ratio` | `0.1` | Minimum λ₂/λ₁ ratio |
| `promax_m` | `4` | Power for Promax |

## Conclusion

The `DivisiveClusterer` offers a complementary approach to the `KMeansClusterer`:

- **Determinism**: Ideal for reproducibility and reports
- **Interpretability**: The split history tells a story
- **Flexibility**: Multiple stopping criteria for context adaptation
- **Performance**: Hybrid path optimized by data type

To explore your data, start with `DivisiveClusterer` with `stop_at_kaiser = TRUE` to discover the natural structure, then refine with `KMeansClusterer` if needed.

## References

- Sarle, W. S. (1990). The VARCLUS Procedure. *SAS/STAT User's Guide*, Version 6, Fourth Edition.
- Chavent, M., et al. (2012). ClustOfVar: An R Package for the Clustering of Variables.

```{r session-info}
sessionInfo()
```
