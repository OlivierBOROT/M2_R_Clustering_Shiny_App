---
title: "M2RClust Package Demonstration"
subtitle: "KMeansClusterer for Variable Clustering"
author: "Your Name"
date: "`r Sys.Date()`"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
    theme: united
---

# Introduction

Ce notebook pr√©sente les fonctionnalit√©s principales du package **M2RClust**, notamment l'algorithme **KMeansClusterer** pour le clustering de variables (et non d'observations).

L'algorithme regroupe les variables en clusters en maximisant l'homog√©n√©it√© intra-cluster, mesur√©e par la proportion de variance expliqu√©e par la premi√®re composante principale de chaque cluster.

```{r setup, message=FALSE, warning=FALSE}
# Charger le package
library(M2RClust)

# Packages additionnels pour la d√©monstration
library(ggplot2)
library(knitr)

# Configuration globale
set.seed(42)
```

---

# 1. Pr√©paration des Donn√©es

Nous utilisons le dataset **mtcars** qui contient 11 variables quantitatives sur 32 mod√®les de voitures.

```{r data_preparation}
# Charger les donn√©es
data(mtcars)

# Aper√ßu des donn√©es
kable(head(mtcars), caption = "Aper√ßu du dataset mtcars")

# Dimensions
cat("Dimensions:", nrow(mtcars), "observations x", ncol(mtcars), "variables\n")

# V√©rifier que toutes les variables sont num√©riques
cat("\nTypes de variables:\n")
sapply(mtcars, class)
```

---

# 2. Initialisation du Clusterer

Nous cr√©ons une instance de `KMeansClusterer` avec 3 clusters.

```{r init_clusterer}
# Cr√©er le clusterer
clusterer <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  standardize = TRUE,  # Standardiser les donn√©es (recommand√©)
  max_iter = 100,
  seed = 42,
  n_init = 10,  # Nombre maximum d'initialisations
  init_method = "homogeneity++"  # M√©thode d'initialisation intelligente
)

# Afficher les informations de base
print(clusterer)
```

---

# 3. Ajustement du Mod√®le

Lan√ßons l'algorithme de clustering.

```{r fit_model}
# Ajuster le mod√®le
clusterer$fit()

# Afficher les r√©sultats
print(clusterer)
```

---

# 4. R√©sum√© D√©taill√©

```{r summary}
# R√©sum√© complet avec toutes les m√©triques
clusterer$summary()
```

---

# 5. Extraction des R√©sultats

```{r extract_results}
# Obtenir les r√©sultats sous forme de dataframe
results <- clusterer$get_results()
kable(results, caption = "Assignations des variables aux clusters")

# Statistiques par cluster
cat("\n=== Statistiques par Cluster ===\n")
for (k in 1:3) {
  vars_in_cluster <- results$variable[results$cluster == k]
  cat("\nCluster", k, ":", length(vars_in_cluster), "variables\n")
  cat("Variables:", paste(vars_in_cluster, collapse = ", "), "\n")
}

# Homog√©n√©it√© globale
cat("\n=== Homog√©n√©it√© Globale ===\n")
cat("Score:", round(clusterer$get_homogeneity(), 4), "\n")

# Homog√©n√©it√© par cluster
cat("\n=== Homog√©n√©it√© par Cluster ===\n")
homogeneities <- clusterer$get_cluster_homogeneity()
for (k in 1:3) {
  cat("Cluster", k, ":", round(homogeneities[k], 4), "\n")
}
```

---

# 6. Visualisations

## 6.1 Clustering en 2D (Espace PCA)

```{r plot_2d, fig.width=10, fig.height=8}
# Visualisation principale dans l'espace des 2 premi√®res composantes principales
plot_clustering_2d(
  clusterer,
  main = "Clustering de Variables - mtcars",
  show_centers = TRUE,
  show_labels = TRUE
)
```

## 6.2 Heatmap des Corr√©lations

```{r plot_heatmap, fig.width=10, fig.height=10}
# Heatmap des corr√©lations regroup√©es par cluster
plot_correlation_heatmap(
  clusterer,
  main = "Matrice de Corr√©lation par Cluster",
  reorder = TRUE
)
```

## 6.3 R√©seau de Corr√©lations

```{r plot_network, fig.width=10, fig.height=10}
# Graphe de r√©seau des corr√©lations
plot_network_graph(
  clusterer,
  threshold = 0.5,  # Seuil de corr√©lation
  main = "R√©seau de Corr√©lations entre Variables",
  layout = "fruchterman.reingold"
)
```

## 6.4 Qualit√© des Clusters

```{r plot_quality, fig.width=10, fig.height=6}
# M√©triques de qualit√© des clusters
metrics <- plot_cluster_quality(
  clusterer,
  main = "M√©triques de Qualit√© des Clusters"
)

# Afficher les m√©triques
cat("\n=== Tailles des Clusters ===\n")
print(metrics$sizes)

cat("\n=== Homog√©n√©it√© des Clusters ===\n")
print(metrics$homogeneity)
```

## 6.5 Radar Chart des Profils

```{r plot_radar, fig.width=10, fig.height=10}
# Diagramme radar des profils de clusters
plot_radar_chart(
  clusterer,
  main = "Profils des Clusters (Radar Chart)",
  standardize = TRUE
)
```

## 6.6 Contributions des Variables

```{r plot_contributions, fig.width=12, fig.height=10}
# Contributions des variables √† chaque cluster
plot_variable_contributions(
  clusterer,
  cluster_id = NULL,  # NULL = tous les clusters
  top_n = 5,  # Top 5 variables par cluster
  main = "Top 5 Variables Contributives par Cluster"
)
```

## 6.7 Scree Plots par Cluster

```{r plot_scree, fig.width=12, fig.height=8}
# Variance expliqu√©e par composante principale dans chaque cluster
plot_scree_by_cluster(
  clusterer,
  main = "Variance Expliqu√©e par Cluster"
)
```

---

# 7. Pr√©diction sur Nouvelles Variables

Nous pouvons projeter de nouvelles variables (suppl√©mentaires) dans l'espace de clustering existant.

```{r predict}
# Cr√©er des variables suppl√©mentaires (corr√©l√©es aux variables existantes)
# Exemple: nouvelles combinaisons lin√©aires
supp_data <- data.frame(
  hp_wt_ratio = mtcars$hp / mtcars$wt,
  disp_cyl_ratio = mtcars$disp / mtcars$cyl
)

cat("Variables suppl√©mentaires:\n")
kable(head(supp_data), caption = "Aper√ßu des variables suppl√©mentaires")

# Pr√©dire les clusters pour ces nouvelles variables
predictions <- clusterer$predict(supp_data, return_scores = TRUE)
kable(predictions, caption = "Pr√©dictions pour les variables suppl√©mentaires")
```

## 7.1 Visualisation avec Variables Suppl√©mentaires

```{r plot_with_supp, fig.width=10, fig.height=8}
# Pr√©parer les coordonn√©es de visualisation pour les variables suppl√©mentaires
clusterer$prepare_plot_predict(supp_data)

# Visualiser avec les variables suppl√©mentaires
plot_clustering_with_supp(
  clusterer,
  main = "Clustering avec Variables Suppl√©mentaires",
  show_centers = TRUE,
  show_labels = TRUE
)
```

---

# 8. S√©lection Automatique du Nombre de Clusters

Le package fournit plusieurs m√©thodes pour d√©terminer le nombre optimal de clusters.

## 8.1 M√©thode du Coude (Elbow)

```{r elbow_method, fig.width=10, fig.height=6}
elbow_results <- elbow_method(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  standardize = TRUE,
  seed = 42
)

cat("\nK sugg√©r√© par la m√©thode du coude:", elbow_results$suggested_k, "\n")
```

## 8.2 M√©thode de la Silhouette

```{r silhouette_method, fig.width=10, fig.height=6}
silhouette_results <- silhouette_method(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  standardize = TRUE,
  seed = 42
)

cat("\nK sugg√©r√© par la m√©thode silhouette:", silhouette_results$suggested_k, "\n")
cat("Score silhouette:", round(silhouette_results$silhouette_scores[silhouette_results$suggested_k - 1], 4), "\n")
```

## 8.3 Indice de Calinski-Harabasz

```{r ch_method, fig.width=10, fig.height=6}
ch_results <- calinski_harabasz_method(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  standardize = TRUE,
  seed = 42
)

cat("\nK sugg√©r√© par Calinski-Harabasz:", ch_results$suggested_k, "\n")
cat("Score CH:", round(ch_results$ch_scores[ch_results$suggested_k - 1], 2), "\n")
```

## 8.4 Comparaison de Toutes les M√©thodes (3 m√©thodes compl√©mentaires)

```{r compare_methods, fig.width=12, fig.height=10}
# Comparer toutes les m√©thodes en une seule fois
comparison <- compare_k_selection_methods(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  # standardize = TRUE,
  seed = 42
)

cat("\n=== Consensus ===\n")
cat("K recommand√© (consensus):", comparison$consensus_k, "\n")

cat("\n=== Toutes les Recommandations ===\n")
print(comparison$all_suggestions)
```

### üìä Interpr√©tation des R√©sultats

**Note importante** : Les diff√©rentes m√©thodes peuvent sugg√©rer des K diff√©rents. Dans notre cas :
- La m√©thode du coude peut sugg√©rer K=3 ou K=4
- Silhouette et Calinski-Harabasz tendent vers K=2
- Davies-Bouldin peut sugg√©rer des K plus √©lev√©s

Pour ce **notebook p√©dagogique**, nous utilisons **K=3** pour illustrer les fonctionnalit√©s, m√™me si ce n'est pas n√©cessairement l'optimum selon toutes les m√©triques. 

En pratique, le choix final de K doit prendre en compte :
1. Les m√©triques statistiques (consensus)
2. L'interpr√©tabilit√© des clusters
3. Le contexte m√©tier

### üî¨ Note M√©thodologique : Approche ClustOfVar

Les m√©triques de validation (Silhouette, Calinski-Harabasz, Davies-Bouldin) dans ce package utilisent l'**approche ClustOfVar** (Chavent et al., 2012) :

- **Distance utilis√©e** : $d(X_i, X_j) = \sqrt{2(1-\text{cor}(X_i, X_j))}$
- **Espace de calcul** : Variables dans $\mathbb{R}^n$ (espace des observations standardis√©es)
- **Avantages** : 
  - Distance euclidienne garantie (m√©trique)
  - Aucune perte d'information (pas de r√©duction dimensionnelle)
  - Coh√©rence avec la litt√©rature sur le clustering de variables

Cette approche diff√®re des visualisations (qui utilisent la projection PCA 2D pour des raisons de lisibilit√©), mais garantit des scores plus fiables et significatifs.

---

# 9. Comparaison de Diff√©rentes M√©thodes d'Initialisation

```{r compare_init, fig.width=12, fig.height=6}
# M√©thode 1: homogeneity++ (intelligent)
clust_hpp <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  standardize = TRUE,
  seed = 42,
  init_method = "homogeneity++"
)
clust_hpp$fit()

# M√©thode 2: correlation (hi√©rarchique)
clust_cor <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  standardize = TRUE,
  seed = 42,
  init_method = "correlation"
)
clust_cor$fit()

# M√©thode 3: random (al√©atoire)
clust_rand <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  standardize = TRUE,
  seed = 42,
  init_method = "random"
)
clust_rand$fit()

# Comparaison des homog√©n√©it√©s
cat("=== Comparaison des M√©thodes d'Initialisation ===\n")
cat("Homogeneity++:  ", round(clust_hpp$get_homogeneity(), 4), "\n")
cat("Correlation:    ", round(clust_cor$get_homogeneity(), 4), "\n")
cat("Random:         ", round(clust_rand$get_homogeneity(), 4), "\n")
```

---

# 10. Export des R√©sultats

```{r export}
# Exporter les r√©sultats dans un fichier CSV
output_file <- tempfile(fileext = ".csv")
clusterer$save_results(output_file)

cat("R√©sultats export√©s vers:", output_file, "\n")

# Lire et afficher le fichier export√©
exported_data <- read.csv(output_file)
kable(head(exported_data), caption = "Donn√©es export√©es")
```

---

# 11. Cas d'Usage Avanc√©: Analyse de la Stabilit√©

```{r stability_analysis}
# Ex√©cuter le clustering plusieurs fois avec diff√©rentes seeds
n_runs <- 10
stability_results <- list()

for (i in 1:n_runs) {
  temp_clust <- KMeansClusterer$new(
    data = mtcars,
    n_clusters = 3,
    standardize = TRUE,
    seed = i
  )
  temp_clust$fit()
  
  stability_results[[i]] <- list(
    seed = i,
    homogeneity = temp_clust$get_homogeneity(),
    iterations = temp_clust$get_iterations(),
    clusters = temp_clust$get_results()$cluster
  )
}

# Analyser la stabilit√©
homogeneities <- sapply(stability_results, function(x) x$homogeneity)
cat(sprintf("=== Analyse de Stabilit√© (%d runs) ===\n", n_runs))
cat("Homog√©n√©it√© moyenne:", round(mean(homogeneities), 4), "\n")
cat("√âcart-type:", round(sd(homogeneities), 4), "\n")
cat("Min:", round(min(homogeneities), 4), "\n")
cat("Max:", round(max(homogeneities), 4), "\n")

# Visualiser la distribution des homog√©n√©it√©s
hist(homogeneities, 
     main = "Distribution de l'Homog√©n√©it√© sur 10 runs",
     xlab = "Homog√©n√©it√©",
     col = "skyblue",
     border = "white")
abline(v = mean(homogeneities), col = "red", lwd = 2, lty = 2)
legend("topright", legend = "Moyenne", col = "red", lwd = 2, lty = 2)
```

---

# 12. Limitations et Interpr√©tation Critique

## 12.1 Pourquoi Certains Scores Peuvent √ätre Faibles ou Nuls

### Scores Silhouette et Calinski-Harabasz Proches de 0

Ces m√©triques peuvent donner des scores faibles pour plusieurs raisons :

1. **Taille limit√©e du dataset** : mtcars n'a que 11 variables
2. **Espace PCA 2D** : Nous utilisons seulement PC1 et PC2 pour les calculs
3. **Clusters non-sph√©riques** : Les variables peuvent former des structures non-lin√©aires
4. **Chevauchement naturel** : Certaines variables sont corr√©l√©es √† plusieurs clusters

```{r interpretation_scores}
# V√©rifier la qualit√© des projections PCA
pca_global <- clusterer$get_pca()
var_explained <- summary(pca_global)$importance[2, 1:2]

cat("Variance expliqu√©e par PC1:", round(var_explained[1] * 100, 2), "%\n")
cat("Variance expliqu√©e par PC2:", round(var_explained[2] * 100, 2), "%\n")
cat("Variance totale (PC1+PC2):", round(sum(var_explained) * 100, 2), "%\n\n")

cat("üí° Interpr√©tation:\n")
cat("Si PC1+PC2 < 60%, les m√©triques calcul√©es en 2D peuvent √™tre trompeuses.\n")
cat("Les variables perdent beaucoup d'information dans la projection 2D.\n")
```

### Comparaison avec un Clustering Al√©atoire

Pour valider que notre clustering est meilleur qu'un hasard :

```{r random_baseline}
# G√©n√©rer 100 clusterings al√©atoires
set.seed(123)
n_simulations <- 100
random_homogeneities <- numeric(n_simulations)

for (i in 1:n_simulations) {
  # Assigner al√©atoirement 11 variables √† 3 clusters
  random_clusters <- sample(1:3, 11, replace = TRUE)
  
  # Calculer l'homog√©n√©it√© approximative
  total_homog <- 0
  for (k in 1:3) {
    vars_k <- which(random_clusters == k)
    if (length(vars_k) > 1) {
      cluster_data <- mtcars[, vars_k]
      pca_k <- prcomp(cluster_data, scale. = TRUE)
      var_exp <- summary(pca_k)$importance[2, 1]
      total_homog <- total_homog + length(vars_k) * var_exp
    }
  }
  random_homogeneities[i] <- total_homog / 11
}

# Comparer
fitted_homog <- clusterer$get_homogeneity()
random_mean <- mean(random_homogeneities)
random_sd <- sd(random_homogeneities)

cat("=== Comparaison avec Clustering Al√©atoire ===\n")
cat("Homog√©n√©it√© du mod√®le:", round(fitted_homog, 4), "\n")
cat("Homog√©n√©it√© baseline (al√©atoire):", round(random_mean, 4), "¬±", round(random_sd, 4), "\n")

z_score <- (fitted_homog - random_mean) / random_sd
cat("Z-score:", round(z_score, 2), "\n\n")

if (z_score > 2) {
  cat("‚úÖ Le mod√®le est significativement meilleur qu'un clustering al√©atoire.\n")
} else {
  cat("‚ö†Ô∏è Le mod√®le n'est pas tr√®s diff√©rent d'un clustering al√©atoire.\n")
  cat("   Cela peut √™tre d√ª √† la petite taille du dataset ou √† l'absence de structure claire.\n")
}
```

---

# 13. Conclusion

Ce notebook a pr√©sent√© les principales fonctionnalit√©s du package **M2RClust** :

1. ‚úÖ Cr√©ation et ajustement d'un mod√®le `KMeansClusterer`
2. ‚úÖ Extraction et analyse des r√©sultats
3. ‚úÖ Visualisations multiples (2D, heatmap, r√©seau, radar, etc.)
4. ‚úÖ Pr√©diction sur nouvelles variables
5. ‚úÖ M√©thodes de s√©lection automatique de K
6. ‚úÖ Comparaison des m√©thodes d'initialisation
7. ‚úÖ Export des r√©sultats
8. ‚úÖ Analyse de stabilit√©

## Points Cl√©s

- **Clustering de variables** (pas d'observations) bas√© sur la corr√©lation
- **Homog√©n√©it√©** mesur√©e par la variance expliqu√©e par PC1
- **M√©thodes d'initialisation** : homogeneity++, correlation, random
- **Visualisations riches** pour interpr√©ter les r√©sultats
- **Outils de validation** pour choisir le nombre de clusters

## Ressources

- Documentation compl√®te : `?KMeansClusterer`
- M√©thodes de s√©lection K : `?compare_k_selection_methods`
- Visualisations : `?visualization`

---

**Session Info**

```{r session_info}
sessionInfo()
```

