---
title: "Complete Guide to KMeansClusterer"
subtitle: "Variable Clustering by Maximizing Within-Cluster Homogeneity"
author: "M2RClust Team"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Complete Guide to KMeansClusterer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)
```

## Introduction

The `KMeansClusterer` is a **variable clustering** algorithm (not observations) that groups variables into homogeneous clusters. Unlike classical K-Means which partitions individuals, this algorithm partitions the columns of a dataset.

### Algorithm Principle

Cluster homogeneity is measured by the **proportion of variance explained by its first principal component**. The algorithm iterates between:
 
1. **Update centers**: compute the 1st principal component (PC1) of each cluster
2. **Reassign variables**: each variable joins the cluster with which it has the highest association (R² for quantitative, η² for qualitative)

### Use Cases

- Interpretable dimensionality reduction
- Identification of correlated variable groups
- Data preparation for multivariate analyses
- Selection of representative variables per cluster

## Installation and Loading

```{r load-package}
library(M2RClust)
```

## Example Data

We use the `mtcars` dataset which contains 11 variables describing 32 automobiles.

```{r data-exploration}
data(mtcars)
str(mtcars)

# Correlation overview
round(cor(mtcars), 2)
```

## Basic Usage

### Creating and Fitting the Model

```{r basic-usage}
# Create the clusterer
clusterer <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  standardize = TRUE,
  seed = 42
)

# Fit the model
clusterer$fit()

# Display results
clusterer$print()
```

### Detailed Summary

```{r summary}
clusterer$summary()
```

### Retrieving Assignments

```{r get-results}
results <- clusterer$get_results()
print(results)

# Table by cluster
for (k in 1:clusterer$n_clusters) {
  vars <- results$variable[results$cluster == k]
  cat(sprintf("\nCluster %d: %s\n", k, paste(vars, collapse = ", ")))
}
```

## Initialization Methods

The `KMeansClusterer` offers three initialization methods:

### 1. Homogeneity++ (default)

Similar to K-Means++, selects diversified initial variables.

```{r init-homogeneitypp}
clust_hpp <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  init_method = "homogeneity++",
  seed = 42
)
clust_hpp$fit()
cat("Homogeneity (homogeneity++):", round(clust_hpp$get_homogeneity(), 4), "\n")
```

### 2. Correlation (hierarchical)

Uses hierarchical clustering on the correlation-based dissimilarity matrix.

```{r init-correlation}
clust_cor <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  init_method = "correlation",
  seed = 42
)
clust_cor$fit()
cat("Homogeneity (correlation):", round(clust_cor$get_homogeneity(), 4), "\n")
```

### 3. Random

Simple random initialization.

```{r init-random}
clust_rand <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  init_method = "random",
  n_init = 20, # More attempts since less stable
  seed = 42
)
clust_rand$fit()
cat("Homogeneity (random):", round(clust_rand$get_homogeneity(), 4), "\n")
```

### Comparing Methods

```{r compare-init}
cat("\n=== Initialization Methods Comparison ===\n")
cat(sprintf(
  "homogeneity++ : %.4f (runs: %d)\n",
  clust_hpp$get_homogeneity(), clust_hpp$get_actual_runs()
))
cat(sprintf(
  "correlation   : %.4f (runs: %d)\n",
  clust_cor$get_homogeneity(), clust_cor$get_actual_runs()
))
cat(sprintf(
  "random        : %.4f (runs: %d)\n",
  clust_rand$get_homogeneity(), clust_rand$get_actual_runs()
))
```

## Visualizations

### Correlation Circle

```{r plot-2d, fig.cap="Correlation circle with clusters"}
plot_clustering_2d(clusterer,
  main = "Variable Clustering - mtcars",
  show_centers = TRUE,
  show_labels = TRUE
)
```

### Correlation Heatmap

```{r plot-heatmap, fig.cap="Correlation heatmap ordered by cluster"}
plot_correlation_heatmap(clusterer, reorder = TRUE)
```

### Cluster Quality

```{r plot-quality, fig.cap="Cluster size and homogeneity"}
quality <- plot_cluster_quality(clusterer)
print(quality)
```

### Network Graph

```{r plot-network, fig.cap="Correlation network between variables"}
# Requires the igraph package
if (requireNamespace("igraph", quietly = TRUE)) {
  plot_network_graph(clusterer, threshold = 0.5, layout = "fruchterman.reingold")
}
```

### Scree Plot by Cluster

```{r plot-scree, fig.cap="Variance explained by component in each cluster"}
plot_scree_by_cluster(clusterer)
```

## Prediction on New Variables

The `KMeansClusterer` allows projecting new (supplementary) variables onto the existing clustering.

```{r predict}
# Create new variables (simulated)
set.seed(123)
new_vars <- data.frame(
  NewVar1 = mtcars$mpg + rnorm(32, sd = 2), # Correlated with mpg
  NewVar2 = mtcars$hp * 0.8 + rnorm(32, sd = 10) # Correlated with hp
)

# Predict clusters
predictions <- clusterer$predict(new_vars, return_scores = TRUE)
print(predictions)
```

### Visualization with Supplementary Variables

```{r plot-predict, fig.cap="Correlation circle with supplementary variables"}
plot_clustering_with_supp(clusterer)
```

## Selecting the Number of Clusters

### Elbow Method

```{r elbow, fig.cap="Elbow method"}
elbow_res <- elbow_method(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  standardize = TRUE,
  seed = 42
)
cat("\nSuggested K (Elbow):", elbow_res$suggested_k, "\n")
```

### Silhouette Method

```{r silhouette, fig.cap="Silhouette method"}
sil_res <- silhouette_method(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  standardize = TRUE,
  seed = 42
)
cat("\nSuggested K (Silhouette):", sil_res$suggested_k, "\n")
```

### Calinski-Harabasz Index

```{r calinski, fig.cap="Calinski-Harabasz index (Pseudo-F)"}
ch_res <- calinski_harabasz_method(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  standardize = TRUE,
  seed = 42
)
cat("\nSuggested K (Calinski-Harabasz):", ch_res$suggested_k, "\n")
```

### Comparing All Methods

```{r compare-k, fig.width=12, fig.height=4, fig.cap="Comparison of K selection methods"}
comparison <- compare_k_selection_methods(
  KMeansClusterer,
  mtcars,
  k_range = 2:8,
  plot = TRUE,
  standardize = TRUE,
  seed = 42
)
cat("\n=== Consensus ===\n")
cat("Recommended K:", comparison$consensus_k, "\n")
```

## Mixed Data (Quantitative + Qualitative)

The `KMeansClusterer` supports mixed data through PCAmix.

```{r mixed-data}
# Create mixed data
mtcars_mixed <- mtcars[, c("mpg", "cyl", "hp", "wt", "qsec")]
mtcars_mixed$cyl <- as.factor(mtcars_mixed$cyl)
mtcars_mixed$gear <- as.factor(mtcars$gear)
mtcars_mixed$am <- as.factor(mtcars$am)

str(mtcars_mixed)

# Clustering on mixed data
if (requireNamespace("PCAmixdata", quietly = TRUE)) {
  clusterer_mixed <- KMeansClusterer$new(
    data = mtcars_mixed,
    n_clusters = 3,
    standardize = TRUE,
    seed = 42
  )
  clusterer_mixed$fit()
  clusterer_mixed$print()

  # Visualization
  plot_clustering_2d(clusterer_mixed, main = "Clustering - Mixed Data")
}
```

## Advanced Parameters

### Convergence Control

```{r advanced-params}
clusterer_advanced <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  standardize = TRUE,
  max_iter = 200, # Maximum iterations per run
  tol = 1e-6, # Stricter convergence tolerance
  n_init = 30, # More initializations
  init_method = "homogeneity++",
  seed = 42
)
clusterer_advanced$fit()

cat("Iterations:", clusterer_advanced$get_iterations(), "\n")
cat(
  "Runs performed:", clusterer_advanced$get_actual_runs(), "/",
  clusterer_advanced$get_n_init(), "\n"
)
cat("Homogeneity:", round(clusterer_advanced$get_homogeneity(), 4), "\n")
```

### Early Stopping

The algorithm automatically stops if the same optimum is found twice (likely global optimum).

```{r early-stop}
# With many initializations, early stopping is often triggered
clusterer_es <- KMeansClusterer$new(
  data = mtcars,
  n_clusters = 3,
  n_init = 50,
  seed = 42
)
clusterer_es$fit()

if (clusterer_es$get_actual_runs() < 50) {
  cat("Early stopping triggered after", clusterer_es$get_actual_runs(), "runs\n")
}
```

## Best Practices

### 1. Standardization

Always standardize data (default) unless variables are already on the same scale.

### 2. Choosing K

- Use `compare_k_selection_methods()` for an overview
- Prioritize interpretability over mathematical optimum

### 3. Reproducibility

Always set a seed (`seed`) for reproducible results.

### 4. Validation

Verify that clusters make business sense by examining:
- Member variables in each cluster
- Cluster homogeneity
- Intra and inter-cluster correlations

## Metrics and Interpretation

### Homogeneity

Cluster homogeneity measures the proportion of variance explained by PC1:
- **1.0**: Perfect homogeneity (all variables perfectly correlated)
- **< 0.5**: Heterogeneous cluster (variables weakly related to each other)

```{r interpret-homogeneity}
cat("Homogeneity by cluster:\n")
homog <- clusterer$get_cluster_homogeneity()
for (k in 1:length(homog)) {
  cat(sprintf("  Cluster %d: %.3f\n", k, homog[k]))
}
cat(sprintf(
  "\nGlobal homogeneity (weighted average): %.3f\n",
  clusterer$get_homogeneity()
))
```

## Conclusion

The `KMeansClusterer` is a powerful tool for grouping correlated variables. It is particularly useful for:

- Exploring the structure of multivariate datasets
- Interpretable dimensionality reduction
- Identifying thematic groups of variables

For data with a natural hierarchical structure, also consider the `DivisiveClusterer` presented in the dedicated vignette.

## References

- Vigneau, E., & Qannari, E. M. (2003). Clustering of variables around latent components. *Communications in Statistics-Simulation and Computation*, 32(4), 1131-1150.
- Chavent, M., Kuentz, V., Liquet, B., & Saracco, L. (2012). ClustOfVar: An R Package for the Clustering of Variables. *Journal of Statistical Software*, 50(13), 1-16.

```{r session-info}
sessionInfo()
```
